{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Romanvia93/traffic_sign_detection/blob/main/yolo/notebooks/Yolov5_w_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Traffic Sign Detection Project Report\n",
        "* Thien An Trinh\n",
        "* Roman Burekhin\n",
        "* Athira Devan\n",
        "* Lester Azinge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Abstract\n",
        "\n",
        "This project aims to train, benchmark and deloy an object detection model to detect 4 types of traffic signs: `traffic light`, `stop`, `speed limit`, and `crosswalk`. The chosen models for the project were `YOLOv5`, `EfficientDet D1`, `SSD MobileNet FPNLite`, `SSD ResNet50 FPN`, and `Faster R-CNN ResNet50`. There are two frameworks for model training, evaluation and inference: `YOLOv5` belongs to [Ultralytics](https://github.com/ultralytics/yolov5) and the rest belong to [Tensorflow](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). Our experiments showed that among the tested models, `YOLOv5` is the best - It won in all criteria including precision, speed, size, and training time, and therefore  was chosen for a video inference (available in `yolo/videos` directory) and a [Streamlit deployment](https://trafficsigns.streamlit.app/). Besides, among the TensorFlow models, `SSD MobileNet FPNLite` is the best model. Hence, it was chosen to run a realtime webcam test on a local machine. The screen recording of this demo is available in `tensorflow/videos` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [
          "noshow"
        ]
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "import os.path\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "import gdown\n",
        "\n",
        "# from collections import defaultdict\n",
        "# import shutil\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# from PIL import Image\n",
        "\n",
        "# import cv2\n",
        "# import random\n",
        "\n",
        "# import yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1. Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are living in a world that is moving towards automation. From robot arms assembling individual components into complete cars to smart household appliances that have been transforming our homes, the benefits of autonomous applications are undeniable. The automobile industry is following the same trend. Not only autopilot systems assist drivers by bringing them better driving experience, but they can also help reduce the number of accidents. For example, a vehicle with a smart traffic sign detection system can “see” all the signs ahead including those the driver could miss, and thereby perform proper actions timely in time-sensitive situation.\n",
        "\n",
        "In that context, our project focused two paramount objectives: \n",
        "1. to meticulously **benchmark** a range of model architectures to identify the most optimal and efficient solution for this challenging task  \n",
        "<br>\n",
        "2. to **deploy** and demonstrate precise and reliable detection of critical traffic signs.\n",
        "\n",
        "Before proceeding into further detail, it is crucial to address certain concepts related to how the task was framed:\n",
        "1. The project is a `computer vision` task – a domain in which images are processed and analyzed in order to extract useful information that can drive decision-making (Arabnia et al., 2018; Yoshida, 2011).  \n",
        "<br>\n",
        "2. This project is specifically an `object detection` task where an image was analyzed not for obtaining the semantic meaning of the whole image (i.e., `image classification`), or for segmenting the image into meaningful regions (i.e., `image segmentation`), but rather to identify targeted objects that are present in the images and determine where on the images they are located. In this project, the objects of interest were `traffic lights`, `stop` signs, `speed limit` signs and `crosswalk` signs, which are the fundamental elements that guide drivers and traffic flow. Third, the algorithms required for this task were defined to be *deep learning (DL)  convolutional neural networks (CNNs)* which has always been the state-of-the-art in the domain for a decade.  \n",
        "<br>\n",
        "3. The project also leveraged an advanced DL technique called transfer learning, in which neural networks that were pretrained on a large dataset are fine-tuned on the dataset of interest instead of being trained from scratch, and therefore are capable of attaining high evaluation scores in the new domain. The details of the data, pretrained-models, training and evaluation frameworks are now discussed in the following section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2. Related Work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The authors took [this YOLOv5 turorial](https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb) and [this TensorFlow tutorial](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html) as the starting point for the work of this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
